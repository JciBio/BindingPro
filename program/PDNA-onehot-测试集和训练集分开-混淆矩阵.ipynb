{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#           1.导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: UTF-8\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from Bio import SeqIO\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.定义CNN的共享函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义权值\n",
    "def weight(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.01), name='weight')\n",
    "\n",
    "#定义偏置\n",
    "def bias(shape):\n",
    "    return tf.Variable(tf.constant(0.01, shape=shape), name='bias')\n",
    "\n",
    "#定义卷积操作\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='VALID')\n",
    "\n",
    "#定义最大池化操作\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "#定义平均池化操作\n",
    "def avg_pool_2x2(x):\n",
    "    return tf.nn.avg_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.定义卷积神经网络\n",
    "  - 返回测试集预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(x_train, x_test, y_train, y_test):\n",
    "    X_TRAIN_SIZE = int(x_train.shape[0])\n",
    "    TRAIN_EPOCHES = 50\n",
    "    BATCH_SIZE = 100\n",
    "    TOTAL_BATCH = int( np.ceil( X_TRAIN_SIZE / BATCH_SIZE))\n",
    "    \n",
    "    x_train = np.reshape(x_train,[-1,31,21,1])\n",
    "    x_test = np .reshape(x_test,[-1,31,21,1])\n",
    "    #输入层 31-by-21\n",
    "    with tf.name_scope('input_layer'):\n",
    "        x = tf.placeholder(tf.float32, shape=[None, 31, 21, 1], name='x')\n",
    "\n",
    "    #第1个卷积层\n",
    "    with tf.name_scope('conv_1'):\n",
    "        W1 = weight([21,21,1,128])\n",
    "        b1 = bias([128])\n",
    "        conv_1 = conv2d(x, W1) + b1\n",
    "        conv_2 = tf.nn.relu(conv_1)\n",
    "\n",
    "    #第1个池化层 16-by-11\n",
    "    with tf.name_scope('pool_1'):\n",
    "        pool_1 = max_pool_2x2(conv_1)\n",
    "\n",
    "#     #第2个卷积层\n",
    "#     with tf.name_scope('conv_2'):\n",
    "#         W2 = weight([5,5,32,64])\n",
    "#         b2 = bias([64])\n",
    "#         conv_2 = conv2d(pool_1, W2) + b2\n",
    "#         conv_2 = tf.nn.relu(conv_2)\n",
    "\n",
    "#     #第2个池化层 8-by-6\n",
    "#     with tf.name_scope(\"pool_2\"):\n",
    "#         pool_2 = max_pool_2x2(conv_2)\n",
    "\n",
    "#     #第3个卷积层\n",
    "#     with tf.name_scope('conv_3'):\n",
    "#         W3 = weight([3,3,64,128])\n",
    "#         b3 = bias([128])\n",
    "#         conv_3 = conv2d(pool_2, W3) + b3\n",
    "#         conv_3 = tf.nn.relu(conv_3)\n",
    "\n",
    "#     #第3个池化层 4-by-3\n",
    "#     with tf.name_scope('pool_3'):\n",
    "#         pool_3 = max_pool_2x2(conv_3)\n",
    "\n",
    "    #全连接层\n",
    "    with tf.name_scope('fc'):\n",
    "        #将最后一个池化层的128个通道的4-by-3的图像转换为一维向量，长度是128*4*3=1536\n",
    "#         W4 = weight([1536,256]) #全连接层定义256个神经元\n",
    "#         b4 = bias([256])\n",
    "#         flat = tf.reshape(pool_3, [-1, 1536])\n",
    "#         h = tf.nn.relu(tf.matmul(flat, W4)) + b4\n",
    "        \n",
    "        W4 = weight([6*1*128,256]) #全连接层定义256个神经元\n",
    "        b4 = bias([256])\n",
    "        flat = tf.reshape(pool_1, [-1, 6*1*128])\n",
    "        h = tf.nn.relu(tf.matmul(flat, W4)) + b4\n",
    "        \n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        h_dropout = tf.nn.dropout(h, keep_prob)\n",
    "\n",
    "    #输出层\n",
    "    with tf.name_scope('output_layer'):\n",
    "        W5 = weight([256,2])\n",
    "        b5 = bias([2])\n",
    "        pred = tf.nn.softmax(tf.matmul(h_dropout, W5) + b5)\n",
    "    \n",
    "    #构建网络模型\n",
    "    with tf.name_scope(\"optimizer\"):\n",
    "        #定义占位符\n",
    "        y = tf.placeholder(tf.int32, shape=[None, 2], name=\"label\")\n",
    "        #定义损失函数\n",
    "        loss_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred,\n",
    "                                                                                 labels=y))\n",
    "        \n",
    "        #选择优化器\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss_function)\n",
    "    \n",
    "    #定义准确率\n",
    "    with tf.name_scope(\"evalulation\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "    #训练模型\n",
    "    epoch= tf.Variable(0, name='epoch', trainable=False)\n",
    "    STARTTIME = time()\n",
    "    start_epoch = 0\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        # 设置检查点存储目录\n",
    "        ckpt_dir = \"log/\"\n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.makedirs(ckpt_dir)\n",
    "        #生成saver\n",
    "        saver = tf.train.Saver(max_to_keep=5)\n",
    "        # 创建 summary_writer，用于写图文件\n",
    "        summary_writer = tf.summary.FileWriter(ckpt_dir, sess.graph)\n",
    "        # 如果有检查点文件，读取最新的检查点文件，恢复各种变量值\n",
    "        ckpt = tf.train.latest_checkpoint(ckpt_dir )\n",
    "        if ckpt != None:\n",
    "            saver.restore(sess, ckpt)     \n",
    "        else:\n",
    "            print(\"Training from scratch.\")\n",
    "\n",
    "        start_epoch= sess.run(epoch)\n",
    "        print(\"Training starts form {} epoch.\".format(start_epoch+1))\n",
    "        \n",
    "        #迭代训练\n",
    "        for ep in range(start_epoch, start_epoch + TRAIN_EPOCHES):\n",
    "            x_train, y_train = shuffle(x_train, y_train)\n",
    "            for i in range(TOTAL_BATCH):\n",
    "                for i in range(a):\n",
    "\n",
    "                    batch_x = names['X_train_%s' % i]\n",
    "                    batch_y = names['Y_train_%s' % i]\n",
    "                    \n",
    "                    sess.run(optimizer,feed_dict={x: batch_x, y: batch_y, keep_prob:0.5})\n",
    "                    if i % 100 == 0:\n",
    "                        print(\"Step {}\".format(i), \"finished\")\n",
    "\n",
    "            loss,acc = sess.run([loss_function,accuracy],feed_dict={x:names['X_test_%s' % i], y:names['Y_test_%s' % i], keep_prob:1.0})\n",
    "            \n",
    "            print(\"Train epoch:\", '%02d' % (sess.run(epoch)+1), \\\n",
    "                  \"Loss=\",\"{:.6f}\".format(loss),\" Accuracy=\",acc)\n",
    "\n",
    "            #保存检查点\n",
    "            #saver.save(sess,ckpt_dir+\"DBPSite_cnn_model.cpkt\",global_step=ep+1)\n",
    "\n",
    "            sess.run(epoch.assign(ep+1))\n",
    "    \n",
    "        duration =time()-STARTTIME\n",
    "        print(\"Train finished takes:\",duration)   \n",
    "    \n",
    "        #计算测试集上的预测结果\n",
    "        y_pred = sess.run(pred, feed_dict={x:X_test, y:Y_test, keep_prob:1.0})\n",
    "        \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.使用K-Fold交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对数据集X按KFold划分训练集和测试集\n",
    "def splitByKFold(X, n=5):\n",
    "    x_trains = []\n",
    "    x_tests = []\n",
    "    kf = KFold(n_splits=n)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        x_train, x_test = X[train_index], X[test_index]\n",
    "        x_trains.append(x_train)\n",
    "        x_tests.append(x_test)\n",
    "     \n",
    "    return x_trains, x_tests\n",
    "\n",
    "#把正类样本集和负类样本集分别做K-Fold划分训练集和测试集\n",
    "#其中正类样本集是少数类集，对其做上采样\n",
    "#然后把对应的正类样本和负类样本保存在名为\"fold_k.npz\"的文件中\n",
    "#fold_k.npz文件中保存以下数据：\n",
    "#    traindata: 训练样本集 \n",
    "#    testdata: 测试样本集\n",
    "#    traintarget: 训练样本分类标签\n",
    "#    testtarget: 测试样本分类标签\n",
    "def saveKFoldSMoteData(positive_data, negative_data, n=5):\n",
    "    for i in range(5):\n",
    "        x_trains_pos, x_tests_pos = splitByKFold(positive_data, n)\n",
    "        x_trains_neg, x_tests_neg = splitByKFold(negative_data, n)\n",
    "        x_train_pos = x_trains_pos[i] #正类训练数据\n",
    "        x_train_neg = x_trains_neg[i] #负类训练数据\n",
    "        x_test_pos = x_tests_pos[i] #正类测试数据\n",
    "        x_test_neg = x_tests_neg[i] #负类测试数据\n",
    "\n",
    "        x_train_pos = SMote(x_train_pos) #正类训练数据是少数类样本，进行上采样\n",
    "\n",
    "        k_train_pos = int( len( x_train_pos)) #正类训练样本数\n",
    "        k_train_neg = int( len( x_train_neg)) #负类训练样本数\n",
    "        k_test_pos = int( len( x_test_pos)) #正类测试样本数\n",
    "        k_test_neg = int( len( x_test_neg)) #负类测试样本数\n",
    "\n",
    "        y_train_pos = np.tile([1,0], (k_train_pos,1))\n",
    "        y_train_neg = np.tile([0,1], (k_train_neg,1))\n",
    "        y_test_pos = np.tile([1,0], (k_test_pos,1))\n",
    "        y_test_neg = np.tile([0,1], (k_test_neg,1))\n",
    "\n",
    "        x_train = np.append(x_train_pos, x_train_neg, axis=0)\n",
    "        y_train = np.append(y_train_pos, y_train_neg, axis=0)\n",
    "\n",
    "        x_test = np.append(x_test_pos, x_test_neg, axis=0)\n",
    "        y_test = np.append(y_test_pos, y_test_neg, axis=0)\n",
    "\n",
    "        x_train = x_train.astype(np.float32)\n",
    "        x_test = x_test.astype(np.float32)\n",
    "\n",
    "        filename = 'fold_' + str(i) + '.npz'\n",
    "        np.savez(filename, traindata=x_train, testdata=x_test, traintarget=y_train, testtarget=y_test)\n",
    "        \n",
    "def validationKFold(n=5):      \n",
    "    y_logists = np.ndarray((0,2))\n",
    "    y_preds = np.ndarray((0,2))\n",
    "    for i in range(n):\n",
    "        filename =  'fold_' + str(i) + '.npz'\n",
    "        D = np.load(filename)\n",
    "        x_train = D['traindata']\n",
    "        x_test = D['testdata']\n",
    "        y_train = D['traintarget']\n",
    "        y_test = D['testtarget']\n",
    "        y_pred = cnn(x_train, x_test, y_train, y_test)\n",
    "        \n",
    "        y_logists = np.append(y_logists, y_test, axis=0)\n",
    "        y_preds = np.append(y_preds, y_pred, axis=0)\n",
    "        \n",
    "    return y_logists, y_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.计算预测率、混淆矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对数据集X按KFold划分训练集和测试集\n",
    "def splitByKFold(X, n=5):\n",
    "    x_trains = []\n",
    "    x_tests = []\n",
    "    kf = KFold(n_splits=n)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        x_train, x_test = X[train_index], X[test_index]\n",
    "        x_trains.append(x_train)\n",
    "        x_tests.append(x_test)\n",
    "     \n",
    "    return x_trains, x_tests\n",
    "\n",
    "#把正类样本集和负类样本集分别做K-Fold划分训练集和测试集\n",
    "#其中正类样本集是少数类集，对其做上采样\n",
    "#然后把对应的正类样本和负类样本保存在名为\"fold_k.npz\"的文件中\n",
    "#fold_k.npz文件中保存以下数据：\n",
    "#    traindata: 训练样本集 \n",
    "#    testdata: 测试样本集\n",
    "#    traintarget: 训练样本分类标签\n",
    "#    testtarget: 测试样本分类标签\n",
    "def saveKFoldSMoteData(positive_data, negative_data, n=5):\n",
    "    for i in range(5):\n",
    "        x_trains_pos, x_tests_pos = splitByKFold(positive_data, n)\n",
    "        x_trains_neg, x_tests_neg = splitByKFold(negative_data, n)\n",
    "        x_train_pos = x_trains_pos[i] #正类训练数据\n",
    "        x_train_neg = x_trains_neg[i] #负类训练数据\n",
    "        x_test_pos = x_tests_pos[i] #正类测试数据\n",
    "        x_test_neg = x_tests_neg[i] #负类测试数据\n",
    "\n",
    "        x_train_pos = SMote(x_train_pos) #正类训练数据是少数类样本，进行上采样\n",
    "\n",
    "        k_train_pos = int( len( x_train_pos)) #正类训练样本数\n",
    "        k_train_neg = int( len( x_train_neg)) #负类训练样本数\n",
    "        k_test_pos = int( len( x_test_pos)) #正类测试样本数\n",
    "        k_test_neg = int( len( x_test_neg)) #负类测试样本数\n",
    "\n",
    "        y_train_pos = np.tile([1,0], (k_train_pos,1))\n",
    "        y_train_neg = np.tile([0,1], (k_train_neg,1))\n",
    "        y_test_pos = np.tile([1,0], (k_test_pos,1))\n",
    "        y_test_neg = np.tile([0,1], (k_test_neg,1))\n",
    "\n",
    "        x_train = np.append(x_train_pos, x_train_neg, axis=0)\n",
    "        y_train = np.append(y_train_pos, y_train_neg, axis=0)\n",
    "\n",
    "        x_test = np.append(x_test_pos, x_test_neg, axis=0)\n",
    "        y_test = np.append(y_test_pos, y_test_neg, axis=0)\n",
    "\n",
    "        x_train = x_train.astype(np.float32)\n",
    "        x_test = x_test.astype(np.float32)\n",
    "\n",
    "        filename = 'test_fold_' + str(i) + '.npz'\n",
    "        np.savez(filename, traindata=x_train, testdata=x_test, traintarget=y_train, testtarget=y_test)\n",
    "        \n",
    "def validationKFold(n=5):      \n",
    "    y_logists = np.ndarray((0,2))\n",
    "    y_preds = np.ndarray((0,2))\n",
    "    for i in range(n):\n",
    "        filename =  'test_fold_' + str(i) + '.npz'\n",
    "        D = np.load(filename)\n",
    "        x_train = D['traindata']\n",
    "        x_test = D['testdata']\n",
    "        y_train = D['traintarget']\n",
    "        y_test = D['testtarget']\n",
    "        y_pred = cnn(x_train, x_test, y_train, y_test)\n",
    "        \n",
    "        y_logists = np.append(y_logists, y_test, axis=0)\n",
    "        y_preds = np.append(y_preds, y_pred, axis=0)\n",
    "        \n",
    "    return y_logists, y_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.计算预测率、混淆矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算预测的准确率\n",
    "def evalution(y_logists, y_preds):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_preds, 1),\n",
    "                                      tf.argmax(y_logists, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "        acc = sess.run(accuracy)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "# 绘制混淆矩阵的函数\n",
    "# 参数1  cm 混淆矩阵中显示的数值 二维数组\n",
    "# 参数2 cmap 混淆矩阵中的颜色\n",
    "# 参数3 title 标题\n",
    "def plot_confusion_matrix(cm, classes, title='混淆矩阵', cmap=plt.cm.Greens):\n",
    "    # imshow() 表示绘制并显示二维图 有18个参数\n",
    "    # 参数1 X 混淆矩阵中显示的数值 二维数组\n",
    "    # 参数2 cmap 颜色 plt.cm.Blues表示蓝色 plt.cm.Reds表示红色 plt.cm.Greens表示绿色\n",
    "    # 参数5 interpolation 插值法 一般有如下值\n",
    "    #     nearest 最近邻插值法\n",
    "    #     bilinear 双线性插值法\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    plt.imshow(cm, cmap=cmap, interpolation=\"nearest\")\n",
    "    plt.title(title)  # 标题\n",
    "    plt.colorbar()  # 显示颜色的进度条\n",
    "    tick_marks = np.arange(2)  # [0 1]\n",
    "    plt.xticks(tick_marks, classes)  # 对x轴上分类进行标记\n",
    "    plt.yticks(tick_marks, classes)  # 对y轴上分类进行标记\n",
    "\n",
    "    thresh = np.mean(cm)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(i, j, cm[j][i],\n",
    "                     horizontalalignment='center',\n",
    "                     color='white' if cm[i][j] >= thresh else 'black')\n",
    "\n",
    "    plt.xlabel('预测值')\n",
    "    plt.ylabel('真实值')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.主函数（保存处理后的数据）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#establishBenchmarkOneHot('..\\\\data\\\\PDNA-224.fasta','..\\\\data\\\\PDNA-224-seqs-OneHot.mat')\n",
    "#splitWindows(15, '..\\\\data\\\\PDNA-224-seqs-OneHot.mat', '..\\\\data\\\\PDNA-224-binding-sites.txt', '..\\\\data\\\\PDNA-224-OneHot-15.mat')\n",
    "# bindingsite = sio.loadmat('data\\\\PDNA-224-OneHot-15.mat')\n",
    "# negative_data = bindingsite['negative']\n",
    "# positive_data = bindingsite['positive']\n",
    "\n",
    "negative_data = sio.loadmat('data\\\\PDNA-224-ONEHOT-11-N.mat')\n",
    "positive_data = sio.loadmat('data\\\\PDNA-224-ONEHOT-11-P.mat')\n",
    "n_X = N_data['n_data']\n",
    "n_Y = N_data['n_target']\n",
    "p_X = P_data['p_data']\n",
    "p_Y = P_data['p_target']\n",
    "\n",
    "#随机打乱正样本数据并返回\n",
    "N_p = p_X.shape[0]\n",
    "print(\"N_p \\t\",N_p)\n",
    "indx = list(range(N_p))\n",
    "random.shuffle(indx)\n",
    "p_x=p_X[indx]\n",
    "p_y=p_Y[indx]\n",
    "\n",
    "\n",
    "#随机打乱负样本数据并返回\n",
    "N_n = n_X.shape[0]\n",
    "print(\"N_n \\t\",N_n)\n",
    "indx = list(range(N_n))\n",
    "random.shuffle(indx)\n",
    "n_x=n_X[indx]\n",
    "n_y=n_Y[indx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#从正负样本中各抽出800条数据，得到测试集 1600个\n",
    "\n",
    "\n",
    "X_test = np.vstack((n_X[52770:53570],p_X[2978:3778]))\n",
    "Y_test = np.vstack((n_Y[52770:53570],p_Y[2978:3778]))\n",
    "#随机打乱测试集数据并返回\n",
    "N_test = X_test.shape[0]\n",
    "print(\"N_test \\t\",N_test)\n",
    "indx = list(range(N_test))\n",
    "random.shuffle(indx)\n",
    "X_test=X_test[indx]\n",
    "Y_test=Y_test[indx]\n",
    "X_test = X_test.reshape(len(X_test),-1)\n",
    "Y_test = Y_test.reshape(len(Y_test),-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#处理训练集，正样本数量不变，在剩余52770个负样本中抽取3022个不放回，组成6000条数据为一组的训练集\n",
    "\n",
    "NN = len(n_X[0:52770])\n",
    "print(NN)\n",
    "\n",
    "#负样本批次大小\n",
    "N_size = 3022\n",
    "#计算一共有负样本多少个批次\n",
    "NN_batch = NN // N_size\n",
    "\n",
    "#print(NN_batch)\n",
    "a=0\n",
    "#得到17个数据集，正样本不变，负样本从'PDNA-224-ONEHOT-11-N.mat中抽出3822个不放回\n",
    "for i in range(NN_batch):\n",
    "               \n",
    "        if (i+2)*N_size > NN: #  53570/3022=17.46\n",
    "                X = np.vstack((n_X[i*N_size:NN],p_X[0:2978]))\n",
    "                Y = np.vstack((n_Y[i*N_size:NN],p_Y[0:2978]))\n",
    "        else:\n",
    "                X = np.vstack((n_X[i*N_size:(i+1)*N_size],p_X[0:2978]))\n",
    "                Y = np.vstack((n_Y[i*N_size:(i+1)*N_size],p_Y[0:2978]))\n",
    "                #print(i*N_size)              \n",
    "        #print(len(X),len(Y))\n",
    "        #print('')\n",
    "        \n",
    "        X = X.reshape(len(X),-1)\n",
    "        Y = Y.reshape(len(Y),-1) \n",
    "        #print(X)\n",
    "        #loo = LeaveOneOut()\n",
    "        kf = KFold(n_splits=5)\n",
    "        kf.get_n_splits(X)\n",
    "        for train_index, test_index in kf.split(X):           \n",
    "            #得到85个数据集，保存在动态变量中,例如 X_train_1\n",
    "            \n",
    "            names['X_train_%s' % a],names['X_test_%s' % a]=X[train_index], X[test_index]\n",
    "            names['Y_train_%s' % a],names['Y_test_%s' % a]=Y[train_index], Y[test_index]\n",
    "            a+=1 \n",
    "\n",
    "y_pred = cnn(x_train, x_test, y_train, y_test)\n",
    "\n",
    "#saveKFoldSMoteData(positive_data, negative_data, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.主函数（训练，测试并画出混淆矩阵）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对数据进行训练\n",
    "y_logists, y_preds = validationKFold(5)\n",
    "acc = evalution(y_logists, y_preds)\n",
    "print(\"5-Fold prediction accuracy=%.4f\" % acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#混淆矩阵\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    #print(y_logists)\n",
    "    \n",
    "    a=tf.argmax(y_logists,1)\n",
    "    b=tf.argmax(y_preds,1)\n",
    "    xx=sess.run(a)\n",
    "    yy=sess.run(b)\n",
    "    print(xx,yy)\n",
    "    #cnf_matrix = confusion_matrix(y_logists[:,0], y_preds[:,0])\n",
    "    cnf_matrix = confusion_matrix(xx, yy)\n",
    "    print(cnf_matrix)\n",
    "    recall = cnf_matrix[0][0] / (cnf_matrix[0][0] + cnf_matrix[0][1])\n",
    "    print('recall: ', recall)\n",
    "    plot_confusion_matrix(cnf_matrix, [1, 0], cmap=plt.cm.Reds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "PDNA-onehot-测试集和训练集分开-混淆矩阵\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "names = locals()\n",
    "#load benchmark dataset\n",
    "N_data = sio.loadmat('data\\\\PDNA-224-ONEHOT-11-N.mat')\n",
    "P_data = sio.loadmat('data\\\\PDNA-224-ONEHOT-11-P.mat')\n",
    "n_X = N_data['n_data']\n",
    "n_Y = N_data['n_target']\n",
    "p_X = P_data['p_data']\n",
    "p_Y = P_data['p_target']\n",
    "\n",
    "\n",
    "\n",
    "#随机打乱正样本数据并返回\n",
    "N_p = p_X.shape[0]\n",
    "print(\"N_p \\t\",N_p)\n",
    "indx = list(range(N_p))\n",
    "random.shuffle(indx)\n",
    "p_x=p_X[indx]\n",
    "p_y=p_Y[indx]\n",
    "\n",
    "\n",
    "#随机打乱负样本数据并返回\n",
    "N_n = n_X.shape[0]\n",
    "print(\"N_n \\t\",N_n)\n",
    "indx = list(range(N_n))\n",
    "random.shuffle(indx)\n",
    "n_x=n_X[indx]\n",
    "n_y=n_Y[indx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 得到测试集 1600个\n",
    "\n",
    "X_test = np.vstack((n_X[52770:53570],p_X[2978:3778]))\n",
    "Y_test = np.vstack((n_Y[52770:53570],p_Y[2978:3778]))\n",
    "#随机打乱测试集数据并返回\n",
    "N_test = X_test.shape[0]\n",
    "print(\"N_test \\t\",N_test)\n",
    "indx = list(range(N_test))\n",
    "random.shuffle(indx)\n",
    "X_test=X_test[indx]\n",
    "Y_test=Y_test[indx]\n",
    "X_test = X_test.reshape(len(X_test),-1)\n",
    "Y_test = Y_test.reshape(len(Y_test),-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#处理训练集，正样本数量不变，在剩余52770个负样本中抽取3022个不放回，组成6000条数据为一组的训练集\n",
    "\n",
    "NN = len(n_X[0:52770])\n",
    "print(NN)\n",
    "\n",
    "#负样本批次大小\n",
    "N_size = 3022\n",
    "#计算一共有负样本多少个批次\n",
    "NN_batch = NN // N_size\n",
    "\n",
    "#print(NN_batch)\n",
    "a=0\n",
    "#得到17个数据集，正样本不变，负样本从'PDNA-224-ONEHOT-11-N.mat中抽出3822个不放回\n",
    "for i in range(NN_batch):\n",
    "               \n",
    "        if (i+2)*N_size > NN: #  53570/3022=17.46\n",
    "                X = np.vstack((n_X[i*N_size:NN],p_X[0:2978]))\n",
    "                Y = np.vstack((n_Y[i*N_size:NN],p_Y[0:2978]))\n",
    "        else:\n",
    "                X = np.vstack((n_X[i*N_size:(i+1)*N_size],p_X[0:2978]))\n",
    "                Y = np.vstack((n_Y[i*N_size:(i+1)*N_size],p_Y[0:2978]))\n",
    "                #print(i*N_size)              \n",
    "        #print(len(X),len(Y))\n",
    "        #print('')\n",
    "        \n",
    "        X = X.reshape(len(X),-1)\n",
    "        Y = Y.reshape(len(Y),-1) \n",
    "        #print(X)\n",
    "        #loo = LeaveOneOut()\n",
    "        kf = KFold(n_splits=5)\n",
    "        kf.get_n_splits(X)\n",
    "        for train_index, test_index in kf.split(X):           \n",
    "            #得到85个数据集，保存在动态变量中,例如 X_train_1\n",
    "            \n",
    "            names['X_train_%s' % a],names['X_test_%s' % a]=X[train_index], X[test_index]\n",
    "            names['Y_train_%s' % a],names['Y_test_%s' % a]=Y[train_index], Y[test_index]\n",
    "            a+=1 \n",
    "            \n",
    "y_pred = cnn(x_train, x_test, y_train, y_test)            \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
